# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(paste("Bagging MSE:", bagging_mse))
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Compute MSE
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
bagging_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a linear regression model to the training data
linear_model <- lm(mpg ~ ., data = train)
# Display the summary of the linear model
summary(linear_model)
# Use the linear model to predict the mpg of the test data
linear_predictions <- predict(linear_model, newdata = test)
# Evaluate the performance of the linear regression model
# using the mean squared error (MSE)
linear_mse <- mean((test$mpg - linear_predictions)^2)
print(paste("Linear Regression MSE:", linear_mse))
# Clear workspace
rm(list=ls())
# Load Libraries
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
num_features <- ncol(train) - 1
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Compute MSE
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
bagging_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
shiny::runApp('Viz_Analytics_Final_Project/Viz_Analytics_Final_Project')
runApp('Viz_Analytics_Final_Project/Viz_Analytics_Final_Project')
runApp('Viz_Analytics_Final_Project/Viz_Analytics_Final_Project')
# Clear the work space
rm(list=ls())
# Load Libraries
library(readxl)
library(writexl)
library(dplyr)
library(lubridate)
# Define the path
excel_path_read <- "Crimes.xlsx"
# Read data from excel and map to a dataframe
crime_data <- read_excel(excel_path_read, sheet = 1)
# View the raw data
#View(crime_data)
# Print the Raw Dataset Shape
print(dim(crime_data))
# Get all the column names
column_names <- colnames(crime_data)
# Define the columns to keep
columns_to_keep <-  c("ID","Date","Primary Type", "Description","Arrest", "Latitude","Longitude")
# Drop the unecessary columns
crime_data <- crime_data  %>%
select(all_of(columns_to_keep))
# View data after the columns are dropped
#View(crime_data)
# See the shape of the data
print(dim(crime_data))
# Mutate the time stamp columns
crime_data <- crime_data %>%
mutate(Date = ymd_hms(Date))
# Define the cutoff date for filtering
cutoff_date <- ymd("2023-05-01")
# Drop rows with dates older than the cutoff
crime_data_subset <- crime_data %>%
filter(Date >= cutoff_date)
# View the subsetted data
View(crime_data_subset)
# See the shape of the data
print(dim(crime_data_subset))
# Exporting the data to a Excel File
# Define the path to save the Excel file
excel_path_write <- "crime_data_subset.xlsx"
# Export the data frame to an Excel file
write_xlsx(crime_data_subset, excel_path_write)
runApp('Viz_Analytics_Final_Project/Viz_Analytics_Final_Project')
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- st_read(shapefile_path)
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- read_sf(shapefile_path)
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- read_sf(shapefile_path)
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- read_sf(shapefile_path)
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- read_sf(shapefile_path)
# Load necessary libraries
library(sf)
# Define the path to the shapefile (adjust the path as needed)
shapefile_path <- "chicago.shp"
# Load the shapefile into R
chicago_areas <- read_sf(shapefile_path)
setwd("C:/Users/dchanda/OneDrive - Marquette University/Study/Visual Analytics/Viz_Analytics_Final_Project/Viz_Analytics_Final_Project")
shiny::runApp()
