boost_model <- gbm(x_train,
y_train)
# Load the Library
library(xgboost)
library(caTools)
library(dplyr)
library(caret)
# Set seed for reproducibility
set.seed(42)
# Apply Split
sample_split <- sample.split(Y = iris$Species, SplitRatio = 0.7)
# Get Train and Test Set
train_set <- subset(x = iris, sample_split == TRUE)
test_set <- subset(x = iris, sample_split == FALSE)
y_train <- as.integer(train_set$Species) - 1
y_test <- as.integer(test_set$Species) - 1
X_train <- train_set %>% select(-Species)
X_test <- test_set %>% select(-Species)
# Load the mtcars dataset
data(mtcars)
# Split the dataset into training and testing sets
library(caTools)
set.seed(123)
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
library(gbm)
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train)
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(paste("Bagging MSE:", bagging_mse))
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
# Using randomForest with mtry set to the number of predictors for bagging
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Evaluate the performance of the bagging model
# using the mean squared error (MSE)
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
print(bagging_mse)
# Clear workspace
rm(list=ls())
# Load Libraries
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
num_features <- ncol(train) - 1  # excluding the response variable
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500  # Use a reasonable number of trees
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Compute MSE
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
bagging_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a linear regression model to the training data
linear_model <- lm(mpg ~ ., data = train)
# Display the summary of the linear model
summary(linear_model)
# Use the linear model to predict the mpg of the test data
linear_predictions <- predict(linear_model, newdata = test)
# Evaluate the performance of the linear regression model
# using the mean squared error (MSE)
linear_mse <- mean((test$mpg - linear_predictions)^2)
print(paste("Linear Regression MSE:", linear_mse))
# Clear workspace
rm(list=ls())
# Load Libraries
library(gbm)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a boosting model to the training data
boost <- gbm(mpg ~ ., data = train,
distribution = "gaussian",
n.trees = 1000, shrinkage = 0.01,
interaction.depth = 4,
bag.fraction = 0.7,
n.minobsinnode = 5)
# Use the model to predict the mpg of the test data
predictions <- predict(boost, newdata = test)
# Evaluate the performance of the model
# using the mean squared error
mse <- mean((test$mpg - predictions)^2)
mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a bagging model to the training data
num_features <- ncol(train) - 1
bagging <- randomForest(
mpg ~ .,
data = train,
mtry = num_features,  # Use all features to simulate bagging
ntree = 500
)
# Use the bagging model to predict the mpg of the test data
bagging_predictions <- predict(bagging, newdata = test)
# Compute MSE
bagging_mse <- mean((test$mpg - bagging_predictions)^2)
bagging_mse
# Clear workspace
rm(list=ls())
# Load Libraries
library(caTools)
library(randomForest)
# Set random seed for reproducibility
set.seed(6250)
# Load the data set
data(mtcars)
# Split the data set into training and testing sets
split <- sample.split(mtcars$mpg, SplitRatio = 0.7)
train <- mtcars[split, ]
test <- mtcars[!split, ]
# Fit a Random Forest model to the training data
# Using randomForest with mtry set to a fraction of the total features
num_features <- ncol(train) - 1  # excluding the response variable
mtry <- ceiling(num_features / 3)  # Common setting for random forests
random_forest <- randomForest(
mpg ~ .,
data = train,
mtry = mtry,  # Use a subset of features for each tree
ntree = 500,  # A good number of trees for random forests
importance = TRUE  # Allows for feature importance
)
# Use the Random Forest model to predict the mpg of the test data
rf_predictions <- predict(random_forest, newdata = test)
# Evaluate the performance of the Random Forest model
# using the mean squared error (MSE)
rf_mse <- mean((test$mpg - rf_predictions)^2)
rf_mse
shiny::runApp('Viz_Analytics_Final_Project/Viz_Analytics_Final_Project')
setwd("C:/Users/dchanda/OneDrive - Marquette University/Study/Visual Analytics/Viz_Analytics_Final_Project/Viz_Analytics_Final_Project")
runApp()
runApp()
